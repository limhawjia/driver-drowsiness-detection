{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating generators...\n",
      "Creating model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "strides should be of length 1, 3 or 5 but was 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-155e99b43d49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlsg-venv/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlsg-venv/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlsg-venv/lib/python3.7/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0minner_mask_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_mask_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_uses_learning_phase'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0muses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlsg-venv/lib/python3.7/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[0;32m~/mlsg-venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3715\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3716\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3717\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   3718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtf_data_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NHWC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3719\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# NHWC -> NCHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlsg-venv/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input, filter, padding, strides, dilation_rate, name, data_format, filters, dilations)\u001b[0m\n\u001b[1;32m    892\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlsg-venv/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    954\u001b[0m       \u001b[0mchannel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NC\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m     \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m     \u001b[0mdilations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dilations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlsg-venv/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_get_sequence\u001b[0;34m(value, n, channel_index, name)\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     raise ValueError(\"{} should be of length 1, {} or {} but was {}\".format(\n\u001b[0;32m---> 73\u001b[0;31m         name, n, n + 2, current_n))\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: strides should be of length 1, 3 or 5 but was 2"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Location of frames\n",
    "training_image_src = '/mnt/disks/a/frames'\n",
    "validation_image_src = '/mnt/disks/b/frames'\n",
    "\n",
    "\"\"\"\n",
    "The directory is divided into folders by the candidate number\n",
    "Within each candidate's folder, the frames are further divided by the label\n",
    "Naming convention of the frames is as follows: [candidate number]_[frame_number]_[label]\n",
    "Single digit candidate numbers are padded with a 0\n",
    "Frame numbers are consecutive and not padded\n",
    "Label can be 0, 5 or 10\n",
    "\"\"\"\n",
    "\n",
    "# This function helps to extract data and labels and return it as a Numpy array from a given image file\n",
    "def extract_data_and_label(image_path):\n",
    "    # We use opencv to read the images as grayscale, this will give us the 2d vector of pixels\n",
    "    # Note that it returns a numpy array and not a Python list, but Keras uses Numpy arrays anyway\n",
    "    image = cv2.imread(image_path, cv2.cv2.IMREAD_GRAYSCALE)\n",
    "    # Because some of the images are corrupt, we got to do this\n",
    "    if image is None or image.data is None or image.size == 0:\n",
    "        return None, None\n",
    "\n",
    "    # Scale the images to a fixed size, second argument is the target dimension, chose an arbitrary\n",
    "    # value for now, (100, 100). Additional arguments can be provided to fine-tune the scaling.\n",
    "    image = cv2.resize(image, (100, 100))\n",
    "    image = image / 255\n",
    "\n",
    "    \"\"\"\n",
    "    !!! Should we extract only the faces? By right CNN is supposed to be able to pick out key features\n",
    "    on its own, but this could possibly make it more effective. This can be done using opencv\n",
    "    \"\"\"\n",
    "\n",
    "    # Next is to extract the labels for each image, in our case, it is just the last portion of the filename\n",
    "    file_name = os.path.basename(image_path)\n",
    "    label = int(os.path.splitext(file_name)[0].split('_')[2])\n",
    "    # Convert to 0, 1 - we are only using images with labels 0 and 10 now\n",
    "    label = 0 if label == 0 else 1\n",
    "#     print(image.shape)\n",
    "    return image, label\n",
    "\n",
    "# Time to actually train the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, TimeDistributed, LSTM, Lambda, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "from keras.regularizers import l1\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "\"\"\"\n",
    "Gonna have to do the procesing in batches because the images are too big to fit all on the ram at the same time.\n",
    "To do so, we define a generator function that will help pull data in batches from the disks.\n",
    "\n",
    "https://mc.ai/train-keras-model-with-large-dataset-batch-training/\n",
    "\"\"\"\n",
    "def batch_generator(files, batch_size):\n",
    "    counter = 0\n",
    "    while True:\n",
    "        pixels = []\n",
    "        labels = []\n",
    "\n",
    "        # print('Generating batch...')\n",
    "        while len(pixels) < batch_size:\n",
    "            filename = files[counter]\n",
    "            data, label = extract_data_and_label(filename)\n",
    "            \n",
    "            if data is None and label is None:\n",
    "                counter = (counter + 1) % len(files)\n",
    "                continue\n",
    "\n",
    "            counter = (counter + 1) % len(files)\n",
    "            pixels.append(data)\n",
    "            labels.append(label)\n",
    "\n",
    "        pixels = np.array(pixels)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        \"\"\"\n",
    "        Gotta reformat the data (once again) to a format that the Conv2D layer accepts. Conv2D layer\n",
    "        is just the convulutional layer provided by keras.\n",
    "\n",
    "        The target format is (w, x, y, z) where w is the number of total images, x and y is the shape of each image\n",
    "        and z is 1 which signifies that the images are grayscale\n",
    "        \"\"\"\n",
    "        pixels = pixels.reshape(batch_size, 100, 100, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        We one-hot-encode our labels to create 3 cateogories, 0 being mapped awake, 5 being mapped to normal and 10 being\n",
    "        mapped to sleepy\n",
    "\n",
    "        !!! Perhaps there can be a better way of encoding the output data? Will this method result in a loss of ordinality?\n",
    "        \"\"\"\n",
    "        labels = to_categorical(labels, num_classes=2)\n",
    "        # yield is a Python thing for generators\n",
    "        print(pixels.shape)\n",
    "        yield pixels, labels\n",
    "\n",
    "\n",
    "# Let's instantiate our generators for the training and validation set\n",
    "print('Creating generators...')\n",
    "from functools import cmp_to_key\n",
    "def compare(a, b):\n",
    "    candidate_a, frame_a, label_a = os.path.splitext(os.path.basename(a))[0].split('_')\n",
    "    candidate_b, frame_b, label_b = os.path.splitext(os.path.basename(b))[0].split('_')\n",
    "    if candidate_a != candidate_b:\n",
    "        return int(candidate_a) - int(candidate_b)\n",
    "    elif label_a != label_b:\n",
    "        return int(label_a) - int(label_b)\n",
    "    else:\n",
    "        return int(frame_a) - int(frame_b)\n",
    "\n",
    "\n",
    "training_files = []\n",
    "for root, dirs, files in os.walk(training_image_src):\n",
    "    temp = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        temp.append(file_path)\n",
    "    temp = sorted(temp, key=cmp_to_key(compare))\n",
    "    temp = temp[:300]\n",
    "    training_files = training_files + temp\n",
    "    \n",
    "training_files = list(filter(lambda x: '_5.jpg' not in x, training_files))\n",
    "# for f in training_files:\n",
    "#     print(f)\n",
    "\n",
    "\n",
    "validation_files = []\n",
    "for root, dirs, files in os.walk(validation_image_src):\n",
    "    temp = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        temp.append(file_path)\n",
    "    temp = sorted(temp, key=cmp_to_key(compare))\n",
    "    temp = temp[:300]\n",
    "    validation_files = validation_files + temp\n",
    "\n",
    "validation_files = list(filter(lambda x: '_5.jpg' not in x, validation_files))\n",
    "# for f in validation_files:\n",
    "#     print(f)\n",
    "\n",
    "batch_size = 300\n",
    "training_generator = batch_generator(training_files, batch_size)\n",
    "validation_generator = batch_generator(validation_files, batch_size)\n",
    "\n",
    "\"\"\"\n",
    "Now we create our model. Keras allows you to build models in a sequential manner or a functional manner. Sequential\n",
    "is easier to understand for me. Its only a syntax difference.\n",
    "\"\"\"\n",
    "print('Creating model...')\n",
    "# model_cnn = Sequential()\n",
    "model = Sequential()\n",
    "\n",
    "\"\"\"\n",
    "The model is essentially what we learnt in the course, a series of layers of neurons and in this case, convulutions.\n",
    "\n",
    "We can tweak the attributes of each layer, such as the size, activation function, etc. This is what they mean by\n",
    "playing with the parameters.\n",
    "\n",
    "I believe what is passed between layers are just Numpy arrays, so what happens is that a layer will take in a Numpy\n",
    "array, transform it using its neurons/convulutions and return the resulting Numpy array.\n",
    "\n",
    "Note that the input shape and output shape of each layer must match.\n",
    "\"\"\"\n",
    "\n",
    "# model_cnn.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(100,100,1),  activity_regularizer=l1(0.001)))\n",
    "# model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model_cnn.add(Conv2D(32, kernel_size=3, activation='relu', activity_regularizer=l1(0.001)))\n",
    "# model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# # Flattens the 2D data into a 1D Numpy array\n",
    "# model_cnn.add(Flatten())\n",
    "\n",
    "# input_lay = Input(shape=(None, 100, 100, 1)) #dimensions of your data\n",
    "# time_distribute = TimeDistributed(Lambda(lambda x: model_cnn(x)))(input_lay) # keras.layers.Lambda is essential to make our trick work :)\n",
    "# lstm_lay = LSTM(10)(time_distribute)\n",
    "# output_lay = Dense(2, activation='softmax')(lstm_lay)\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, kernel_size=3, activation='relu', activity_regularizer=l1(0.001)), input_shape=(300,50,100, 100, 1)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=3, activation='relu', activity_regularizer=l1(0.001))))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "# Flattens the 2D data into a 1D Numpy array\n",
    "# model.add(TimeDistributed(Flatten()))\n",
    "# model.add((LSTM(4, return_sequences=True, dropout=0.5)))\n",
    "\n",
    "# Let's use LSTM instead of a standard MLP layer toend\n",
    "# model.add(LSTM(10, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# model = Model(inputs=[input_lay], outputs=[output_lay])\n",
    "model.compile(optimizer='adam', loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "print('Starting training...')\n",
    "# Training the model only takes a simple function call\n",
    "# Epochs is the number of passes over the dataset we want for the training\n",
    "history = model.fit_generator(training_generator, validation_data=validation_generator, \n",
    "                              epochs=2, steps_per_epoch=np.ceil(len(training_files)/batch_size), \n",
    "                              validation_steps=np.ceil(len(validation_files)/batch_size), \n",
    "                              verbose=1, shuffle=True)\n",
    "\n",
    "\n",
    "# # Save model to json for future use\n",
    "# model_json = model.to_json()\n",
    "# with open(\"cnn.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # Save weights for future use\n",
    "# model.save_weights(\"model.h5\")\n",
    "\n",
    "# You can save model and weights together\n",
    "model.save(\"cnnlstm.h5\");\n",
    "\n",
    "# What follows is just a few library calls to plot the results throughout the course of the training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('cnnlstm_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('cnnlstm_loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsg-venv",
   "language": "python",
   "name": "mlsg-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
