{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mlsg/mlsg-venv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#%tensorflow_version 2.x \n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Activation, Conv2D, Dropout\n",
    "from keras.layers import ConvLSTM2D, MaxPooling2D, Flatten, BatchNormalization, Conv3D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import statistics\n",
    "from scipy.spatial import distance\n",
    "from mlxtend.image import extract_face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract landmarks\n",
    "\n",
    "def get_eyes_aspect_ratio(image_to_process):\n",
    "    landmarks = extract_face_landmarks(image_to_process)\n",
    "    left_eye_aspect_ratio, right_eye_aspect_ratio = calc_eyes_aspect_ratio(landmarks)\n",
    "    return left_eye_aspect_ratio, right_eye_aspect_ratio\n",
    "    \n",
    "def calc_eyes_aspect_ratio(landmarks):\n",
    "    left_eye = []\n",
    "    right_eye = []\n",
    "    for i in range(36, 42):\n",
    "        left_eye.append([landmarks[i][0], landmarks[i][1]])\n",
    "    for i in range(42, 48):\n",
    "        right_eye.append([landmarks[i][0], landmarks[i][1]])\n",
    "    return calc_eye_aspect_ratio(left_eye), calc_eye_aspect_ratio(right_eye)\n",
    "\n",
    "def calc_eye_aspect_ratio(coords):\n",
    "    height1 = distance.euclidean(coords[1], coords[5])\n",
    "    height2 = distance.euclidean(coords[2], coords[4])\n",
    "    length = distance.euclidean(coords[0], coords[3])\n",
    "    return (height1 + height2) / (2 * length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function helps to extract data and labels and return it as a Numpy array from a given image file\n",
    "def extract_data_and_label(image_path):\n",
    "    # We use opencv to read the images as grayscale, this will give us the 2d vector of pixels\n",
    "    # Note that it returns a numpy array and not a Python list, but Keras uses Numpy arrays anyway\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Because some of the images are corrupt, we got to do this\n",
    "    if image is None or image.data is None or image.size == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    left_eye_aspect_ratio, right_eye_aspect_ratio = get_eyes_aspect_ratio(image)\n",
    "    \n",
    "    '''\n",
    "    # Scale the images to a fixed size, second argument is the target dimension, chose an arbitrary\n",
    "    # value for now, (100, 100). Additional arguments can be provided to fine-tune the scaling.\n",
    "    image = cv2.resize(image, (1,129600))\n",
    "    image = image / 255\n",
    "    '''\n",
    "\n",
    "    # Next is to extract the labels for each image, in our case, it is just the last portion of the filename\n",
    "    file_name = os.path.basename(image_path)\n",
    "    label = int(os.path.splitext(file_name)[0].split('_')[2])\n",
    "    # Convert to 0, 1, 2\n",
    "    label = 0 if label == 0 else 1 if label == 5 else 2\n",
    "\n",
    "    return left_eye_aspect_ratio, right_eye_aspect_ratio, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the training result\n",
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'accuracy' in h:\n",
    "        meas='accuracy'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print image\n",
    "def print_image(image_to_print):\n",
    "    plt.figure()\n",
    "    plt.imshow(image_to_print, cmap='gray') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part for normalised values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85875, 4)\n",
      "(85875,)\n",
      "(59889, 4)\n",
      "(59889,)\n"
     ]
    }
   ],
   "source": [
    "def get_row(row):\n",
    "    result = []\n",
    "    for val in row:\n",
    "        result.append(float(val))\n",
    "    \n",
    "    return result\n",
    "\n",
    "TRAINING_DATA_PATH='/home/mlsg/DiskA/others/ratios'\n",
    "VALIDATION_DATA_PATH='/home/mlsg/DiskB/others/ratios'\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for csv_dir in os.listdir(TRAINING_DATA_PATH):\n",
    "    full_path = TRAINING_DATA_PATH + '/' + csv_dir\n",
    "    endings = ['0', '10']\n",
    "    \n",
    "    for ending in endings:\n",
    "        filename = full_path + '/' + ending + '.csv'\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, mode='r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if (len(row) > 6):\n",
    "                        x_train.append([float(row[5]), float(row[6]), float(row[7]), float(row[8])])\n",
    "                        \n",
    "                        if (int(ending) == 0):\n",
    "                            y_train.append(int(0))\n",
    "                        elif int(ending) == 5:\n",
    "                            y_train.append(int(1))\n",
    "                        elif int(ending) == 10:\n",
    "                            y_train.append(int(1))\n",
    "                            \n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for csv_dir in os.listdir(VALIDATION_DATA_PATH):\n",
    "    full_path = VALIDATION_DATA_PATH + '/' + csv_dir\n",
    "    endings = ['0', '10']\n",
    "    \n",
    "    for ending in endings:\n",
    "        filename = full_path + '/' + ending + '.csv'\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, mode='r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if (len(row) > 6):\n",
    "                        x_test.append([float(row[5]), float(row[6]), float(row[7]), float(row[8])])\n",
    "                        \n",
    "                        if (int(ending) == 0):\n",
    "                            y_test.append(int(0))\n",
    "                        elif int(ending) == 5:\n",
    "                            y_test.append(int(1))\n",
    "                        elif int(ending) == 10:\n",
    "                            y_test.append(int(1))\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85875\n",
      "85875\n",
      "59889\n",
      "59889\n",
      "[ 0.29753638 -0.03287357 -0.09855425 -0.23360286]\n",
      "0\n",
      "0.29753637672852506\n",
      "(85875, 4)\n",
      "(85875,)\n",
      "(59889, 4)\n",
      "(59889,)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_test))\n",
    "print(len(y_test))\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(x_train[0][0])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), 1, 4)\n",
    "x_test = x_test.reshape(len(x_test), 1, 4)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part for the non-generator method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_image_src = '../driver-drowsiness-detection/processing/training_aspect_ratios.csv'\n",
    "validation_image_src = '../driver-drowsiness-detection/processing/validation_aspect_ratios.csv'\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "with open(training_image_src, mode='r') as file:\n",
    "    csv_reader = csv.reader(file, delimiter=',')\n",
    "    row_count = 0\n",
    "    for row in csv_reader:\n",
    "        x_train.append([float(row[0]), float(row[1]), float(row[2]), float(row[3]), float(row[4]), float(row[5])])\n",
    "        if (str(row[6]) == \"2\"):\n",
    "            y_train.append(1)\n",
    "        elif str(row[6]) == \"0\":\n",
    "            y_train.append(0)\n",
    "        \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "with open(validation_image_src, mode='r') as file:\n",
    "    csv_reader = csv.reader(file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        x_test.append([float(row[0]), float(row[1]), float(row[2]), float(row[3]), float(row[4]), float(row[5])])\n",
    "        if (str(row[6]) == \"2\"):\n",
    "            y_test.append(1)\n",
    "        elif str(row[6]) == \"0\":\n",
    "            y_test.append(0)\n",
    "        \n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), 1, 6)\n",
    "x_test = x_test.reshape(len(x_test), 1, 6)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86642, 1, 6)\n",
      "(86642, 2)\n",
      "(23559, 1, 6)\n",
      "(23559, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjusts these parameters to our liking\n",
    "lr = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 100 # How many images we want it to process at any given time\n",
    "\n",
    "n_input_shape = (1, 4)  # data input (img shape: 360*360 flattened to be 129600)\n",
    "n_input_size = 360 #720  #input layer number of neurons\n",
    "n_hidden_1 = 100 #360  #1st layer number of neurons\n",
    "n_hidden_2 = 100#360  #2nd layer number of neurons\n",
    "n_classes = 2\n",
    "\n",
    "regularizer = l2(0.001)\n",
    "opt = Adam() #SGD(lr=learning_rate)\n",
    "loss_fn = 'categorical_crossentropy'\n",
    "\n",
    "# Sequential \n",
    "# Edit the model here\n",
    "model = Sequential(name=\"MLP\")\n",
    "model.add(Dense(n_input_size, activation='relu', name=\"Input\", \n",
    "                activity_regularizer=regularizer, kernel_regularizer=regularizer,\n",
    "                input_shape=(n_input_shape)))\n",
    "model.add(Dropout(rate=0.001))\n",
    "model.add(Dense(n_hidden_1, activation='relu', name=\"Hidden_1\", \n",
    "                activity_regularizer=regularizer, kernel_regularizer=regularizer))\n",
    "model.add(Dropout(rate=0.001))\n",
    "model.add(Dense(n_hidden_2, activation='relu', name=\"Hidden_2\", \n",
    "                activity_regularizer=regularizer, kernel_regularizer=regularizer))\n",
    "model.add(Dropout(rate=0.001))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes, activation='softmax', \n",
    "                activity_regularizer=regularizer, kernel_regularizer=regularizer,\n",
    "                name=\"Output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MLP\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (Dense)                (None, 1, 360)            1800      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 360)            0         \n",
      "_________________________________________________________________\n",
      "Hidden_1 (Dense)             (None, 1, 100)            36100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "Hidden_2 (Dense)             (None, 1, 100)            10100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 48,202\n",
      "Trainable params: 48,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/mlsg/mlsg-venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 85875 samples, validate on 59889 samples\n",
      "Epoch 1/10\n",
      "85875/85875 [==============================] - 6s 67us/step - loss: 0.7222 - accuracy: 0.7528 - val_loss: 0.7334 - val_accuracy: 0.6772\n",
      "Epoch 2/10\n",
      "85875/85875 [==============================] - 4s 52us/step - loss: 0.6272 - accuracy: 0.7605 - val_loss: 0.7385 - val_accuracy: 0.6710\n",
      "Epoch 3/10\n",
      "85875/85875 [==============================] - 4s 52us/step - loss: 0.6248 - accuracy: 0.7606 - val_loss: 0.7148 - val_accuracy: 0.6835\n",
      "Epoch 4/10\n",
      "85875/85875 [==============================] - 4s 52us/step - loss: 0.6242 - accuracy: 0.7607 - val_loss: 0.7093 - val_accuracy: 0.6848\n",
      "Epoch 5/10\n",
      "85875/85875 [==============================] - 4s 52us/step - loss: 0.6229 - accuracy: 0.7618 - val_loss: 0.7126 - val_accuracy: 0.6831\n",
      "Epoch 6/10\n",
      "85875/85875 [==============================] - 4s 52us/step - loss: 0.6227 - accuracy: 0.7622 - val_loss: 0.7170 - val_accuracy: 0.6827\n",
      "Epoch 7/10\n",
      "85875/85875 [==============================] - 4s 51us/step - loss: 0.6224 - accuracy: 0.7626 - val_loss: 0.7188 - val_accuracy: 0.6823\n",
      "Epoch 8/10\n",
      "85875/85875 [==============================] - 4s 51us/step - loss: 0.6221 - accuracy: 0.7617 - val_loss: 0.7100 - val_accuracy: 0.6845\n",
      "Epoch 9/10\n",
      "85875/85875 [==============================] - 4s 51us/step - loss: 0.6215 - accuracy: 0.7627 - val_loss: 0.7123 - val_accuracy: 0.6870\n",
      "Epoch 10/10\n",
      "85875/85875 [==============================] - 4s 51us/step - loss: 0.6218 - accuracy: 0.7627 - val_loss: 0.7188 - val_accuracy: 0.6850\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "model.save(\"mlp.h5\");\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('MLP Model Accuracy with Normalized Facial Landmarks')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('mlp_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('mlp_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    lr = 0.001\n",
    "    training_epochs = 10\n",
    "    batch_size = 100 # How many images we want it to process at any given time\n",
    "\n",
    "    n_input_shape = (1, 4)  # data input (img shape: 360*360 flattened to be 129600)\n",
    "    n_input_size = 360 #720  #input layer number of neurons\n",
    "    n_hidden_1 = 100 #360  #1st layer number of neurons\n",
    "    n_hidden_2 = 100#360  #2nd layer number of neurons\n",
    "    n_classes = 2\n",
    "\n",
    "    regularizer = l2(0.001)\n",
    "    opt = Adam() #SGD(lr=learning_rate)\n",
    "    loss_fn = 'categorical_crossentropy'\n",
    "\n",
    "    # Sequential \n",
    "    # Edit the model here\n",
    "    model = Sequential(name=\"MLP\")\n",
    "    model.add(Dense(n_input_size, activation='relu', name=\"Input\", \n",
    "                    activity_regularizer=regularizer, kernel_regularizer=regularizer,\n",
    "                    input_shape=(n_input_shape)))\n",
    "    model.add(Dropout(rate=0.001))\n",
    "    model.add(Dense(n_hidden_1, activation='relu', name=\"Hidden_1\", \n",
    "                    activity_regularizer=regularizer, kernel_regularizer=regularizer))\n",
    "    model.add(Dropout(rate=0.001))\n",
    "    model.add(Dense(n_hidden_2, activation='relu', name=\"Hidden_2\", \n",
    "                    activity_regularizer=regularizer, kernel_regularizer=regularizer))\n",
    "    model.add(Dropout(rate=0.001))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes, activation='softmax', \n",
    "                    activity_regularizer=regularizer, kernel_regularizer=regularizer,\n",
    "                    name=\"Output\"))\n",
    "    model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_x, train_y, test_x, test_y):\n",
    "    model.fit(train_x, train_y, epochs=30, batch_size=10)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5952 - accuracy: 0.7232\n",
      "Epoch 2/30\n",
      "116611/116611 [==============================] - 27s 228us/step - loss: 0.5802 - accuracy: 0.7287\n",
      "Epoch 3/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5780 - accuracy: 0.7309\n",
      "Epoch 4/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5764 - accuracy: 0.7322\n",
      "Epoch 5/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5763 - accuracy: 0.7323\n",
      "Epoch 6/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5755 - accuracy: 0.7329\n",
      "Epoch 7/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5749 - accuracy: 0.7333\n",
      "Epoch 8/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5746 - accuracy: 0.7337\n",
      "Epoch 9/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5745 - accuracy: 0.7331\n",
      "Epoch 10/30\n",
      "116611/116611 [==============================] - 25s 218us/step - loss: 0.5740 - accuracy: 0.7342\n",
      "Epoch 11/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5737 - accuracy: 0.7344\n",
      "Epoch 12/30\n",
      "116611/116611 [==============================] - 25s 219us/step - loss: 0.5740 - accuracy: 0.7352\n",
      "Epoch 13/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5734 - accuracy: 0.7345\n",
      "Epoch 14/30\n",
      "116611/116611 [==============================] - 26s 227us/step - loss: 0.5734 - accuracy: 0.7349\n",
      "Epoch 15/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5732 - accuracy: 0.7355\n",
      "Epoch 16/30\n",
      "116611/116611 [==============================] - 25s 218us/step - loss: 0.5730 - accuracy: 0.7360\n",
      "Epoch 17/30\n",
      "116611/116611 [==============================] - 25s 217us/step - loss: 0.5725 - accuracy: 0.7369\n",
      "Epoch 18/30\n",
      "116611/116611 [==============================] - 25s 217us/step - loss: 0.5725 - accuracy: 0.7356\n",
      "Epoch 19/30\n",
      "116611/116611 [==============================] - 26s 220us/step - loss: 0.5726 - accuracy: 0.7351\n",
      "Epoch 20/30\n",
      "116611/116611 [==============================] - 26s 225us/step - loss: 0.5727 - accuracy: 0.7349\n",
      "Epoch 21/30\n",
      "116611/116611 [==============================] - 26s 221us/step - loss: 0.5725 - accuracy: 0.7348\n",
      "Epoch 22/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5719 - accuracy: 0.7358\n",
      "Epoch 23/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5723 - accuracy: 0.7356\n",
      "Epoch 24/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5721 - accuracy: 0.7356\n",
      "Epoch 25/30\n",
      "116611/116611 [==============================] - 26s 220us/step - loss: 0.5722 - accuracy: 0.7349\n",
      "Epoch 26/30\n",
      "116611/116611 [==============================] - 27s 227us/step - loss: 0.5723 - accuracy: 0.7362\n",
      "Epoch 27/30\n",
      "116611/116611 [==============================] - 26s 220us/step - loss: 0.5723 - accuracy: 0.7358\n",
      "Epoch 28/30\n",
      "116611/116611 [==============================] - 26s 220us/step - loss: 0.5726 - accuracy: 0.7350\n",
      "Epoch 29/30\n",
      "116611/116611 [==============================] - 25s 218us/step - loss: 0.5725 - accuracy: 0.7357\n",
      "Epoch 30/30\n",
      "116611/116611 [==============================] - 26s 219us/step - loss: 0.5723 - accuracy: 0.7357\n",
      "accuracy: 74.14%\n",
      "Epoch 1/30\n",
      "116611/116611 [==============================] - 27s 229us/step - loss: 0.5948 - accuracy: 0.7240\n",
      "Epoch 2/30\n",
      "116611/116611 [==============================] - 27s 229us/step - loss: 0.5798 - accuracy: 0.7307\n",
      "Epoch 3/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5781 - accuracy: 0.7323\n",
      "Epoch 4/30\n",
      "116611/116611 [==============================] - 26s 223us/step - loss: 0.5773 - accuracy: 0.7323\n",
      "Epoch 5/30\n",
      "116611/116611 [==============================] - 26s 222us/step - loss: 0.5765 - accuracy: 0.7329\n",
      "Epoch 6/30\n",
      "116611/116611 [==============================] - 26s 223us/step - loss: 0.5756 - accuracy: 0.7331\n",
      "Epoch 7/30\n",
      "116611/116611 [==============================] - 26s 223us/step - loss: 0.5749 - accuracy: 0.7335\n",
      "Epoch 8/30\n",
      "116611/116611 [==============================] - 27s 233us/step - loss: 0.5746 - accuracy: 0.7343\n",
      "Epoch 9/30\n",
      "116611/116611 [==============================] - 26s 225us/step - loss: 0.5745 - accuracy: 0.7343\n",
      "Epoch 10/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5740 - accuracy: 0.7348\n",
      "Epoch 11/30\n",
      "116611/116611 [==============================] - 26s 225us/step - loss: 0.5737 - accuracy: 0.7343\n",
      "Epoch 12/30\n",
      "116611/116611 [==============================] - 26s 225us/step - loss: 0.5735 - accuracy: 0.7350\n",
      "Epoch 13/30\n",
      "116611/116611 [==============================] - 27s 230us/step - loss: 0.5734 - accuracy: 0.7345\n",
      "Epoch 14/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5734 - accuracy: 0.7351\n",
      "Epoch 15/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5727 - accuracy: 0.7352\n",
      "Epoch 16/30\n",
      "116611/116611 [==============================] - 26s 225us/step - loss: 0.5727 - accuracy: 0.7354\n",
      "Epoch 17/30\n",
      "116611/116611 [==============================] - 26s 223us/step - loss: 0.5729 - accuracy: 0.7355\n",
      "Epoch 18/30\n",
      "116611/116611 [==============================] - 26s 222us/step - loss: 0.5722 - accuracy: 0.7360\n",
      "Epoch 19/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5722 - accuracy: 0.7356\n",
      "Epoch 20/30\n",
      "116611/116611 [==============================] - 27s 233us/step - loss: 0.5724 - accuracy: 0.7364\n",
      "Epoch 21/30\n",
      "116611/116611 [==============================] - 26s 223us/step - loss: 0.5719 - accuracy: 0.7369\n",
      "Epoch 22/30\n",
      "116611/116611 [==============================] - 26s 223us/step - loss: 0.5718 - accuracy: 0.7365\n",
      "Epoch 23/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5720 - accuracy: 0.7362\n",
      "Epoch 24/30\n",
      "116611/116611 [==============================] - 26s 223us/step - loss: 0.5717 - accuracy: 0.7365\n",
      "Epoch 25/30\n",
      "116611/116611 [==============================] - 27s 231us/step - loss: 0.5716 - accuracy: 0.7366\n",
      "Epoch 26/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5718 - accuracy: 0.7370\n",
      "Epoch 27/30\n",
      "116611/116611 [==============================] - 26s 226us/step - loss: 0.5716 - accuracy: 0.7365\n",
      "Epoch 28/30\n",
      "116611/116611 [==============================] - 26s 226us/step - loss: 0.5717 - accuracy: 0.7355\n",
      "Epoch 29/30\n",
      "116611/116611 [==============================] - 26s 224us/step - loss: 0.5717 - accuracy: 0.7364\n",
      "Epoch 30/30\n",
      "116611/116611 [==============================] - 26s 225us/step - loss: 0.5716 - accuracy: 0.7369\n",
      "accuracy: 73.86%\n",
      "Epoch 1/30\n",
      "116611/116611 [==============================] - 27s 234us/step - loss: 0.5956 - accuracy: 0.7240\n",
      "Epoch 2/30\n",
      "116611/116611 [==============================] - 26s 226us/step - loss: 0.5796 - accuracy: 0.7303\n",
      "Epoch 3/30\n",
      "116611/116611 [==============================] - 26s 223us/step - loss: 0.5772 - accuracy: 0.7318\n",
      "Epoch 4/30\n",
      "116611/116611 [==============================] - 26s 222us/step - loss: 0.5762 - accuracy: 0.7336\n",
      "Epoch 5/30\n",
      "116611/116611 [==============================] - 26s 222us/step - loss: 0.5754 - accuracy: 0.7338\n",
      "Epoch 6/30\n",
      "116611/116611 [==============================] - 27s 228us/step - loss: 0.5747 - accuracy: 0.7343\n",
      "Epoch 7/30\n",
      "116611/116611 [==============================] - 26s 220us/step - loss: 0.5745 - accuracy: 0.7344\n",
      "Epoch 8/30\n",
      "116611/116611 [==============================] - 25s 218us/step - loss: 0.5743 - accuracy: 0.7353\n",
      "Epoch 9/30\n",
      "116611/116611 [==============================] - 27s 234us/step - loss: 0.5737 - accuracy: 0.7351\n",
      "Epoch 10/30\n",
      "116611/116611 [==============================] - 25s 215us/step - loss: 0.5737 - accuracy: 0.7352\n",
      "Epoch 11/30\n",
      "116611/116611 [==============================] - 27s 236us/step - loss: 0.5729 - accuracy: 0.7360\n",
      "Epoch 12/30\n",
      "116611/116611 [==============================] - 27s 235us/step - loss: 0.5729 - accuracy: 0.7356\n",
      "Epoch 13/30\n",
      "116611/116611 [==============================] - 28s 243us/step - loss: 0.5727 - accuracy: 0.7359\n",
      "Epoch 14/30\n",
      "116611/116611 [==============================] - 27s 234us/step - loss: 0.5727 - accuracy: 0.7361\n",
      "Epoch 15/30\n",
      "116611/116611 [==============================] - 27s 234us/step - loss: 0.5722 - accuracy: 0.7362\n",
      "Epoch 16/30\n",
      "116611/116611 [==============================] - 27s 231us/step - loss: 0.5724 - accuracy: 0.7368\n",
      "Epoch 17/30\n",
      "116611/116611 [==============================] - 27s 233us/step - loss: 0.5719 - accuracy: 0.7363\n",
      "Epoch 18/30\n",
      "116611/116611 [==============================] - 28s 239us/step - loss: 0.5717 - accuracy: 0.7369\n",
      "Epoch 19/30\n",
      "116611/116611 [==============================] - 27s 233us/step - loss: 0.5721 - accuracy: 0.7372\n",
      "Epoch 20/30\n",
      "116611/116611 [==============================] - 27s 233us/step - loss: 0.5719 - accuracy: 0.7361\n",
      "Epoch 21/30\n",
      "116611/116611 [==============================] - 27s 234us/step - loss: 0.5717 - accuracy: 0.7360\n",
      "Epoch 22/30\n",
      "116611/116611 [==============================] - 27s 233us/step - loss: 0.5713 - accuracy: 0.7364\n",
      "Epoch 23/30\n",
      "116611/116611 [==============================] - 27s 230us/step - loss: 0.5717 - accuracy: 0.7366\n",
      "Epoch 24/30\n",
      "116611/116611 [==============================] - 28s 242us/step - loss: 0.5716 - accuracy: 0.7366\n",
      "Epoch 25/30\n",
      "116611/116611 [==============================] - 28s 240us/step - loss: 0.5716 - accuracy: 0.7364\n",
      "Epoch 26/30\n",
      "116611/116611 [==============================] - 27s 234us/step - loss: 0.5715 - accuracy: 0.7369\n",
      "Epoch 27/30\n",
      "116611/116611 [==============================] - 28s 244us/step - loss: 0.5714 - accuracy: 0.7367\n",
      "Epoch 28/30\n",
      "116611/116611 [==============================] - 28s 244us/step - loss: 0.5711 - accuracy: 0.7368\n",
      "Epoch 29/30\n",
      "116611/116611 [==============================] - 30s 254us/step - loss: 0.5707 - accuracy: 0.7375\n",
      "Epoch 30/30\n",
      "116611/116611 [==============================] - 27s 234us/step - loss: 0.5709 - accuracy: 0.7381\n",
      "accuracy: 73.78%\n",
      "Epoch 1/30\n",
      "116611/116611 [==============================] - 28s 242us/step - loss: 0.5959 - accuracy: 0.7264\n",
      "Epoch 2/30\n",
      "116611/116611 [==============================] - 29s 245us/step - loss: 0.5791 - accuracy: 0.7310\n",
      "Epoch 3/30\n",
      "116611/116611 [==============================] - 30s 257us/step - loss: 0.5770 - accuracy: 0.7326\n",
      "Epoch 4/30\n",
      "116611/116611 [==============================] - 29s 252us/step - loss: 0.5761 - accuracy: 0.7330\n",
      "Epoch 5/30\n",
      "116611/116611 [==============================] - 32s 270us/step - loss: 0.5754 - accuracy: 0.7345\n",
      "Epoch 6/30\n",
      "116611/116611 [==============================] - 30s 258us/step - loss: 0.5745 - accuracy: 0.7357\n",
      "Epoch 7/30\n",
      "116611/116611 [==============================] - 30s 256us/step - loss: 0.5742 - accuracy: 0.7346\n",
      "Epoch 8/30\n",
      "116611/116611 [==============================] - 30s 256us/step - loss: 0.5739 - accuracy: 0.7358\n",
      "Epoch 9/30\n",
      "116611/116611 [==============================] - 31s 262us/step - loss: 0.5734 - accuracy: 0.7365\n",
      "Epoch 10/30\n",
      "116611/116611 [==============================] - 30s 254us/step - loss: 0.5730 - accuracy: 0.7360\n",
      "Epoch 11/30\n",
      "116611/116611 [==============================] - 29s 249us/step - loss: 0.5723 - accuracy: 0.7376\n",
      "Epoch 12/30\n",
      "116611/116611 [==============================] - 29s 252us/step - loss: 0.5726 - accuracy: 0.7372\n",
      "Epoch 13/30\n",
      "116611/116611 [==============================] - 29s 247us/step - loss: 0.5716 - accuracy: 0.7381\n",
      "Epoch 14/30\n",
      "116611/116611 [==============================] - 30s 253us/step - loss: 0.5722 - accuracy: 0.7373\n",
      "Epoch 15/30\n",
      "116611/116611 [==============================] - 31s 264us/step - loss: 0.5718 - accuracy: 0.7374\n",
      "Epoch 16/30\n",
      "116611/116611 [==============================] - 33s 283us/step - loss: 0.5716 - accuracy: 0.7378\n",
      "Epoch 17/30\n",
      "116611/116611 [==============================] - 32s 276us/step - loss: 0.5719 - accuracy: 0.7370\n",
      "Epoch 18/30\n",
      "116611/116611 [==============================] - 32s 274us/step - loss: 0.5716 - accuracy: 0.7369\n",
      "Epoch 19/30\n",
      "116611/116611 [==============================] - 30s 261us/step - loss: 0.5714 - accuracy: 0.7375\n",
      "Epoch 20/30\n",
      "116611/116611 [==============================] - 30s 254us/step - loss: 0.5712 - accuracy: 0.7378\n",
      "Epoch 21/30\n",
      "116611/116611 [==============================] - 32s 276us/step - loss: 0.5713 - accuracy: 0.7379\n",
      "Epoch 22/30\n",
      "116611/116611 [==============================] - 33s 286us/step - loss: 0.5709 - accuracy: 0.7374\n",
      "Epoch 23/30\n",
      "116611/116611 [==============================] - 34s 295us/step - loss: 0.5710 - accuracy: 0.7382\n",
      "Epoch 24/30\n",
      "116611/116611 [==============================] - 34s 294us/step - loss: 0.5710 - accuracy: 0.7383\n",
      "Epoch 25/30\n",
      "116611/116611 [==============================] - 36s 311us/step - loss: 0.5711 - accuracy: 0.7377\n",
      "Epoch 26/30\n",
      "116611/116611 [==============================] - 36s 308us/step - loss: 0.5709 - accuracy: 0.7384\n",
      "Epoch 27/30\n",
      "116611/116611 [==============================] - 35s 301us/step - loss: 0.5711 - accuracy: 0.7376\n",
      "Epoch 28/30\n",
      "116611/116611 [==============================] - 37s 315us/step - loss: 0.5707 - accuracy: 0.7385\n",
      "Epoch 29/30\n",
      "116611/116611 [==============================] - 32s 271us/step - loss: 0.5714 - accuracy: 0.7378\n",
      "Epoch 30/30\n",
      "116611/116611 [==============================] - 29s 247us/step - loss: 0.5710 - accuracy: 0.7379\n",
      "accuracy: 73.47%\n",
      "Epoch 1/30\n",
      "116612/116612 [==============================] - 30s 254us/step - loss: 0.5931 - accuracy: 0.7272\n",
      "Epoch 2/30\n",
      "116612/116612 [==============================] - 29s 251us/step - loss: 0.5784 - accuracy: 0.7327\n",
      "Epoch 3/30\n",
      "116612/116612 [==============================] - 29s 252us/step - loss: 0.5762 - accuracy: 0.7335\n",
      "Epoch 4/30\n",
      "116612/116612 [==============================] - 29s 249us/step - loss: 0.5750 - accuracy: 0.7346\n",
      "Epoch 5/30\n",
      "116612/116612 [==============================] - 30s 257us/step - loss: 0.5737 - accuracy: 0.7354\n",
      "Epoch 6/30\n",
      "116612/116612 [==============================] - 29s 245us/step - loss: 0.5731 - accuracy: 0.7354\n",
      "Epoch 7/30\n",
      "116612/116612 [==============================] - 27s 234us/step - loss: 0.5728 - accuracy: 0.7361\n",
      "Epoch 8/30\n",
      "116612/116612 [==============================] - 27s 231us/step - loss: 0.5723 - accuracy: 0.7368\n",
      "Epoch 9/30\n",
      "116612/116612 [==============================] - 28s 238us/step - loss: 0.5720 - accuracy: 0.7366\n",
      "Epoch 10/30\n",
      "116612/116612 [==============================] - 27s 230us/step - loss: 0.5716 - accuracy: 0.7371\n",
      "Epoch 11/30\n",
      "116612/116612 [==============================] - 27s 233us/step - loss: 0.5717 - accuracy: 0.7373\n",
      "Epoch 12/30\n",
      "116612/116612 [==============================] - 27s 233us/step - loss: 0.5712 - accuracy: 0.7371\n",
      "Epoch 13/30\n",
      "116612/116612 [==============================] - 27s 233us/step - loss: 0.5713 - accuracy: 0.7375\n",
      "Epoch 14/30\n",
      "116612/116612 [==============================] - 27s 231us/step - loss: 0.5705 - accuracy: 0.7383\n",
      "Epoch 15/30\n",
      "116612/116612 [==============================] - 27s 231us/step - loss: 0.5709 - accuracy: 0.7375\n",
      "Epoch 16/30\n",
      "116612/116612 [==============================] - 28s 241us/step - loss: 0.5707 - accuracy: 0.7380\n",
      "Epoch 17/30\n",
      "116612/116612 [==============================] - 27s 232us/step - loss: 0.5705 - accuracy: 0.7387\n",
      "Epoch 18/30\n",
      "116612/116612 [==============================] - 27s 231us/step - loss: 0.5700 - accuracy: 0.7382\n",
      "Epoch 19/30\n",
      "116612/116612 [==============================] - 27s 228us/step - loss: 0.5705 - accuracy: 0.7380\n",
      "Epoch 20/30\n",
      "116612/116612 [==============================] - 27s 232us/step - loss: 0.5703 - accuracy: 0.7380\n",
      "Epoch 21/30\n",
      "116612/116612 [==============================] - 27s 228us/step - loss: 0.5702 - accuracy: 0.7391\n",
      "Epoch 22/30\n",
      "116612/116612 [==============================] - 26s 226us/step - loss: 0.5699 - accuracy: 0.7383\n",
      "Epoch 23/30\n",
      "116612/116612 [==============================] - 26s 226us/step - loss: 0.5703 - accuracy: 0.7378\n",
      "Epoch 24/30\n",
      "116612/116612 [==============================] - 26s 226us/step - loss: 0.5702 - accuracy: 0.7386\n",
      "Epoch 25/30\n",
      "116612/116612 [==============================] - 27s 228us/step - loss: 0.5701 - accuracy: 0.7377\n",
      "Epoch 26/30\n",
      "116612/116612 [==============================] - 29s 249us/step - loss: 0.5703 - accuracy: 0.7383\n",
      "Epoch 27/30\n",
      "116612/116612 [==============================] - 31s 262us/step - loss: 0.5699 - accuracy: 0.7382\n",
      "Epoch 28/30\n",
      "116612/116612 [==============================] - 29s 250us/step - loss: 0.5703 - accuracy: 0.7386\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116612/116612 [==============================] - 28s 239us/step - loss: 0.5700 - accuracy: 0.7387\n",
      "Epoch 30/30\n",
      "116612/116612 [==============================] - 28s 240us/step - loss: 0.5699 - accuracy: 0.7390\n",
      "accuracy: 73.10%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "labels = np.concatenate([y_train, y_test])\n",
    "data = np.concatenate([x_train, x_test])\n",
    "# labels = data_y.concat(data_x)\n",
    "scores = []\n",
    "n_folds = 5\n",
    "\n",
    "new_labels = []\n",
    "for c in labels:\n",
    "    if c[0] == 0.0:\n",
    "        new_labels.append(1)\n",
    "    else:\n",
    "        new_labels.append(0)\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "for train, test in skf.split(data, new_labels):\n",
    "    eval_model = None # Clearing the NN.\n",
    "    eval_model = new_model()\n",
    "    score = train_and_evaluate(eval_model, data[np.array(train)], labels[np.array(train)], data[np.array(test)], labels[np.array(test)])\n",
    "    scores.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "new_y_test = []\n",
    "for c in y_test:\n",
    "    if c[0] == 0.0:\n",
    "        new_y_test.append(1)\n",
    "    else:\n",
    "        new_y_test.append(0)\n",
    "fpr_mlp, tpr_mlp, thresholds = roc_curve(new_y_test, [c[1] for c in pred])\n",
    "roc_auc = metrics.auc(fpr_mlp, tpr_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'fpr_mlp' (ndarray)\n",
      "Stored 'tpr_mlp' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store fpr_mlp\n",
    "%store tpr_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6aaaceedbc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print(statistics.mean(scores))\n",
    "print(statistics.stdev(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below is for generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gonna have to do the procesing in batches because the images are too big to fit all on the ram at the same time.\n",
    "To do so, we define a generator function that will help pull data in batches from the disks.\n",
    "\n",
    "https://mc.ai/train-keras-model-with-large-dataset-batch-training/\n",
    "\"\"\"\n",
    "def batch_generator(files, batch_size):\n",
    "    counter = 0\n",
    "    while True:\n",
    "        pixels = []\n",
    "        labels = []\n",
    "\n",
    "        # print('Generating batch...')\n",
    "        while len(pixels) < batch_size:\n",
    "            filename = files[counter]\n",
    "            try:\n",
    "                left_eye_aspect_ratio, right_eye_aspect_ratio, label = extract_data_and_label(filename)\n",
    "            except:\n",
    "                counter = (counter + 1) % len(files)\n",
    "                continue\n",
    "            #if data is None and label is None:\n",
    "            if left_eye_aspect_ratio is None or right_eye_aspect_ratio is None or label is None:\n",
    "                counter = (counter + 1) % len(files)\n",
    "                continue\n",
    "\n",
    "            counter = (counter + 1) % len(files)\n",
    "            pixels.append([left_eye_aspect_ratio, right_eye_aspect_ratio])\n",
    "            labels.append(label)\n",
    "\n",
    "        pixels = np.array(pixels)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        pixels = pixels.reshape(batch_size, 1, 2)\n",
    "        \n",
    "        \"\"\"\n",
    "        We one-hot-encode our labels to create 3 cateogories, 0 being mapped awake, 5 being mapped to normal and 10 being\n",
    "        mapped to sleepy\n",
    "\n",
    "        !!! Perhaps there can be a better way of encoding the output data? Will this method result in a loss of ordinality?\n",
    "        \"\"\"\n",
    "        labels = tf.keras.utils.to_categorical(labels, num_classes=3)\n",
    "        # yield is a Python thing for generators\n",
    "        yield pixels, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1) :\n",
    "    pixel, labels = next(batch_generator(validation_files, 100))\n",
    "    print(str(i))\n",
    "    print(\"pixel: \", len(pixel))\n",
    "    print(\"labels: \", len(labels))\n",
    "    print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This part loads the training & validation files' location\n",
    "# Location of frames\n",
    "training_image_src = '../DiskA/others/faces-resized'\n",
    "validation_image_src = '../DiskB/others/faces-resized'\n",
    "\n",
    "training_files = []\n",
    "for root, dirs, files in os.walk(training_image_src):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        training_files.append(file_path)\n",
    "\n",
    "validation_files = []\n",
    "for root, dirs, files in os.walk(validation_image_src):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        validation_files.append(file_path)\n",
    "        \n",
    "# Checking the size to confirm that the files are loaded\n",
    "print(\"size of training files =\", len(training_files))\n",
    "print(\"size of validation files =\", len(validation_files))\n",
    "\n",
    "# image = extract_data_and_label(training_files[2])\n",
    "# print(\"shape of image:\", image[0].shape)\n",
    "\n",
    "#print_image(mpimg.imread(training_files[1]))\n",
    "image1 = cv2.imread(training_files[10], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "left_eye_aspect_ratio, right_eye_aspect_ratio = get_eyes_aspect_ratio(image1)\n",
    "test_pixel = []\n",
    "test_pixel.append([left_eye_aspect_ratio, right_eye_aspect_ratio])\n",
    "\n",
    "image2 = cv2.imread(training_files[50], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "left_eye_aspect_ratio, right_eye_aspect_ratio = get_eyes_aspect_ratio(image2)\n",
    "test_pixel.append([left_eye_aspect_ratio, right_eye_aspect_ratio])\n",
    "test_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "training_sample_size = len(training_files)\n",
    "validation_sample_size = len(validation_files)\n",
    "# Checking the size to confirm that the files are loaded\n",
    "print(\"size of training files =\", str(training_sample_size))\n",
    "print(\"size of validation files =\", str(validation_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_files = random.sample(training_files, training_sample_size)\n",
    "validation_files = random.sample(validation_files, validation_sample_size)\n",
    "\n",
    "# Adjusts these parameters to our liking\n",
    "learning_rate = 0.001\n",
    "training_epochs = 5\n",
    "batch_size = 50 # How many images we want it to process at any given time\n",
    "\n",
    "training_generator = batch_generator(training_files, batch_size)\n",
    "validation_generator = batch_generator(validation_files, batch_size)\n",
    "\n",
    "n_input_shape = (1, 2)  # data input (img shape: 360*360 flattened to be 129600)\n",
    "n_input_size = 720 # input layer number of neurons\n",
    "n_hidden_1 = 360 # 1st layer number of neurons\n",
    "n_hidden_2 = 360 # 2nd layer number of neurons\n",
    "n_classes = 3 # MNIST classes for prediction(digits 0-10)\n",
    "\n",
    "regularizer = l2(0.001)\n",
    "\n",
    "# Sequential \n",
    "# Edit the model here\n",
    "model = Sequential(name=\"MLP\")\n",
    "model.add(Dense(n_input_size, activation='relu', name=\"Input\", \n",
    "                kernel_regularizer=regularizer, input_shape=(n_input_shape)))\n",
    "model.add(Dense(n_hidden_1, activation='relu', name=\"Hidden_1\",\n",
    "                kernel_regularizer=regularizer))\n",
    "model.add(Dense(n_hidden_2, activation='relu', name=\"Hidden_2\",\n",
    "                kernel_regularizer=regularizer))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_regularizer=regularizer, \n",
    "                name=\"Output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit_generator(training_generator, \n",
    "                              validation_data=validation_generator, \n",
    "                              epochs=training_epochs, \n",
    "                              steps_per_epoch=np.ceil(len(training_files)/batch_size), \n",
    "                              validation_steps=np.ceil(len(validation_files)/batch_size), \n",
    "                              verbose=1, \n",
    "                              shuffle=True)\n",
    "\n",
    "model.save(\"mlp.h5\");\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('mlp_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('mlp_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN ANYTHING ELSE FROM HERE ON\n",
    "'''\n",
    "# training_image_name_pixel_label = {}\n",
    "# validation_image_name_pixel_label = {}\n",
    "\n",
    "# loads the images into arrays\n",
    "x_train = [] # actual pixel values of training images\n",
    "y_train = [] # label / classification\n",
    "x_test = []\n",
    "y_test = []\n",
    "'''\n",
    "\n",
    "for filename in glob.glob(training_image_src + '*.jpg'):\n",
    "    im=cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    #print(im.shape)\n",
    "    #print(im)\n",
    "    #plt.imshow(im, cmap='gray')\n",
    "    label = filename[len(filename) - 6: len(filename) - 4]\n",
    "    if label == \"_5\":\n",
    "      label = str(0)\n",
    "    elif label == \"_0\":\n",
    "      label = str(1)\n",
    "    elif label == \"10\":\n",
    "      label = str(2)\n",
    "    im= im/255\n",
    "    im.resize(129600) #uncomment this if dont have to flatten it\n",
    "    x_train.append(im)\n",
    "    y_train.append(int(label))\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "'''\n",
    "for filename in glob.glob(validation_image_src + '*.jpg'):\n",
    "    im=cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    #print(im.shape)\n",
    "    #print(im)\n",
    "    #plt.imshow(im, cmap='gray')\n",
    "    label = filename[len(filename) - 6: len(filename) - 4]\n",
    "    if label == \"_0\":\n",
    "      label = str(0)\n",
    "    elif label == \"_5\":\n",
    "      label = str(1)\n",
    "    elif label == \"10\":\n",
    "      label = str(2)\n",
    "    im= im/255\n",
    "    im.resize(129600) #uncomment this if dont have to flatten it\n",
    "    x_test.append(im)\n",
    "    y_test.append(int(label))\n",
    "    #image_name_pixel_label[filename] = (im, label)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "n_classes = 3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list(image_name_pixel_label.values())[0][0].shape\n",
    "print(\"len(x_train):\", len(x_train))\n",
    "print(\"x_train.shape:\", x_train.shape)\n",
    "print(\"len(y_train):\", len(y_train))\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"len(x_test):\", len(x_test))\n",
    "print(\"len(y_test):\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print(\"x_train.shape\", x_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"x_test.shape\", x_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "batch_size = 50 # How many images we want it to process at any given time\n",
    "\n",
    "n_input_shape = 129600  # data input (img shape: 360*360 flattened to be 129600)\n",
    "n_input_size = 720 # input layer number of neurons\n",
    "n_hidden_1 = 360 # 1st layer number of neurons\n",
    "n_hidden_2 = 360 # 2nd layer number of neurons\n",
    "n_classes = 3 # MNIST classes for prediction(digits 0-10)\n",
    "\n",
    "regularizer = l2(0.001)\n",
    "\n",
    "# Sequential \n",
    "model = Sequential(name=\"MLP\")\n",
    "model.add(Dense(n_input_size, activation='relu', name=\"Input\", \n",
    "                kernel_regularizer=regularizer, input_shape=(n_input_shape,)))\n",
    "model.add(Dense(n_hidden_1, activation='relu', name=\"Hidden_1\",\n",
    "                kernel_regularizer=regularizer))\n",
    "model.add(Dense(n_hidden_2, activation='relu', name=\"Hidden_2\",\n",
    "                kernel_regularizer=regularizer))\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_regularizer=regularizer, \n",
    "                name=\"Output\"))\n",
    "\n",
    "'''\n",
    "# Functional\n",
    "Inp = Input(shape = (n_input_shape,))\n",
    "x = Dense(n_hidden_1, activation='relu', name = \"Dense_1\")(Inp)\n",
    "x = Dense(n_hidden_2, activation='relu', name = \"Dense_2\")(x)\n",
    "output = Dense(n_classes, activation='softmax', name = \"OutputLayer\")(x)\n",
    "model = Model(Inp, output, name = \"our_dense_model\")\n",
    "'''\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = SGD(lr=learning_rate)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Plot graph\n",
    "plot_train(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsg-venv",
   "language": "python",
   "name": "mlsg-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
